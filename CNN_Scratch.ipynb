{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_Scratch",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvmYH0GG48rH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAZpWhspR3cE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir('drive/My Drive/Dataset')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dOgGgSzXStMb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def unpickle(file):\n",
        "  import pickle\n",
        "  with open(file, 'rb') as fo:\n",
        "      dict = pickle.load(fo, encoding='bytes')\n",
        "  return dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxAvP7EMSS2m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Load_Data():\n",
        "  \n",
        "  train_set_X = []\n",
        "  train_set_Y = []\n",
        "  test_set_X = []\n",
        "  test_set_Y = []\n",
        "  \n",
        "  for i in range(1,6):\n",
        "    filename = 'data_batch_'+str(i)\n",
        "    dict = unpickle(filename)\n",
        "    X = dict[b'data']\n",
        "    Y = dict[b'labels']\n",
        "    train_set_X.extend(X)\n",
        "    train_set_Y.extend(Y)\n",
        "    \n",
        "  train_set_X = np.array(train_set_X)\n",
        "  train_set_Y = np.array(train_set_Y)\n",
        "  \n",
        "  \n",
        "  filename = 'test_batch'\n",
        "  dict = unpickle(filename)\n",
        "  test_set_X = np.array(dict[b'data'])\n",
        "  test_set_Y = np.array(dict[b'labels'])\n",
        "  \n",
        "  \n",
        "  filename = 'batches.meta'\n",
        "  dict = unpickle(filename)\n",
        "  classes = np.array(dict[b'label_names'])\n",
        "\n",
        "  #print(train_set_X.shape,train_set_Y.shape,test_set_X.shape,test_set_Y.shape,classes.shape)\n",
        "  return train_set_X,train_set_Y,test_set_X,test_set_Y,classes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OmN-BGX4Z37T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Reshape_And_Normalize(train_set_X,train_set_Y,test_set_X,test_set_Y):\n",
        "  \n",
        "  m_train = train_set_X.shape[0]\n",
        "  m_test = test_set_X.shape[0]\n",
        "  U_train = np.sum(train_set_X,axis=1,keepdims=True)/m_train\n",
        "  U_test = np.sum(test_set_X,axis=1,keepdims=True)/m_test\n",
        "  \n",
        "  train_set_X = train_set_X - U_train\n",
        "  test_set_X = test_set_X - U_test\n",
        "  sigma_train = np.sqrt(np.sum(np.square(train_set_X),axis=1,keepdims=True)/m_train)\n",
        "  sigma_test = np.sqrt(np.sum(np.square(test_set_X),axis=1,keepdims=True)/m_test)\n",
        "  \n",
        "  train_set_X = train_set_X/sigma_train\n",
        "  test_set_X = test_set_X/sigma_test\n",
        "  \n",
        "  train_set_X = train_set_X.reshape(train_set_X.shape[0],3,32,32)\n",
        "  train_set_X = np.moveaxis(train_set_X,1,3)\n",
        "  train_set_Y = train_set_Y.reshape(1,train_set_Y.shape[0])\n",
        "  test_set_X = test_set_X.reshape(test_set_X.shape[0],3,32,32)\n",
        "  test_set_X = np.moveaxis(test_set_X,1,3)\n",
        "  test_set_Y = test_set_Y.reshape(1,test_set_Y.shape[0])\n",
        "  \n",
        "  train_set_X = train_set_X[0:50,:,:,:]\n",
        "  test_set_X = test_set_X[0:10,:,:,:]\n",
        "  train_set_Y = train_set_Y[:,0:50]\n",
        "  test_set_Y = test_set_Y[:,0:10]\n",
        "  \n",
        "  #print(train_set_X.shape,train_set_Y.shape,test_set_X.shape,test_set_Y.shape)\n",
        "  return train_set_X,train_set_Y,test_set_X,test_set_Y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkmBugB3SinF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Data_Preprocessing():\n",
        "  \n",
        "  train_set_X,train_set_Y,test_set_X,test_set_Y,classes = Load_Data()\n",
        "  train_set_X,train_set_Y,test_set_X,test_set_Y = Reshape_And_Normalize(train_set_X,train_set_Y,test_set_X,test_set_Y)\n",
        "  return train_set_X,train_set_Y,test_set_X,test_set_Y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bb4sGIKRWNYr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        },
        "outputId": "a47ead0e-01ae-41c2-d751-1533dcee1cd5"
      },
      "source": [
        "train_set_X,train_set_Y,test_set_X,test_set_Y,classes = Load_Data()\n",
        "train_set_X = train_set_X.reshape(train_set_X.shape[0],3,32,32)\n",
        "train_set_X = np.moveaxis(train_set_X,1,3)\n",
        "print(train_set_X.shape)\n",
        "train_set_Y =train_set_Y.reshape(1,train_set_Y.shape[0])\n",
        "i = 1\n",
        "plt.imshow(train_set_X[i])\n",
        "print('y = '+str(train_set_Y[0,i])+'. It is a '+classes[train_set_Y[0,i]].decode('utf-8')+' picture')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50000, 32, 32, 3)\n",
            "y = 9. It is a truck picture\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKAp+IRKKgl+IhKLgFyKhKPiFSCj/D/EcoRMKOMsFAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTFHGaGXbzZ0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Initialize_Parameters(layers_dims,filter_conv,channels):\n",
        "  \n",
        "  parameters={}\n",
        "  L = len(layers_dims)\n",
        "  for l in range(1,L):\n",
        "    WF = np.random.randn(layers_dims[l],layers_dims[l-1])*np.sqrt(1/layers_dims[l-1])*np.sqrt(1/layers_dims[l-1])\n",
        "    bF = np.zeros((layers_dims[l],1))\n",
        "    YF = np.random.randn(layers_dims[l],1)*np.sqrt(1/layers_dims[l-1])*np.sqrt(1/layers_dims[l-1])\n",
        "    BF = np.zeros((layers_dims[l],1))\n",
        "    \n",
        "    parameters['WF'+str(l)] = WF \n",
        "    parameters['bF'+str(l)] = bF\n",
        "    parameters['YF'+str(l)] = YF\n",
        "    parameters['BF'+str(l)] = BF\n",
        "  \n",
        "  L = len(channels)\n",
        "  for l in range(1,L):\n",
        "    WC = np.random.randn(channels[l],filter_conv[l-1][0],filter_conv[l-1][1],channels[l-1])*np.sqrt(1/channels[l-1])\n",
        "    bC = np.zeros((channels[l],1,1,1))\n",
        "    \n",
        "    parameters['WC'+str(l)] = WC\n",
        "    parameters['bC'+str(l)] = bC\n",
        "  \n",
        "  return parameters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLCxeot-psgz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Initialize_Optimizer(layers_dims,parameters,filter_conv,channels):\n",
        "  \n",
        "  v = {} \n",
        "  s = {}\n",
        "  L = len(layers_dims)\n",
        "  \n",
        "  for l in range(1,L):\n",
        "    dWF = np.zeros(parameters['WF'+str(l)].shape)\n",
        "    dbF = np.zeros(parameters['bF'+str(l)].shape)\n",
        "    dYF = np.zeros(parameters['YF'+str(l)].shape)\n",
        "    dBF = np.zeros(parameters['BF'+str(l)].shape)\n",
        "\n",
        "    s['dWF'+str(l)] = v['dWF'+str(l)] = dWF\n",
        "    s['dbF'+str(l)] = v['dbF'+str(l)] = dbF\n",
        "    s['dYF'+str(l)] = v['dYF'+str(l)] = dYF\n",
        "    s['dBF'+str(l)] = v['dBF'+str(l)] = dBF\n",
        "    \n",
        "  L=len(channels)\n",
        "  \n",
        "  for l in range(1,L):\n",
        "    dWC = np.zeros((channels[l],filter_conv[l-1][0],filter_conv[l-1][1],channels[l-1]))\n",
        "    dbC = np.zeros((channels[l],1,1,1))\n",
        "    \n",
        "    s['dWC'+str(l)] = v['dWC'+str(l)] = dWC\n",
        "    s['dbC'+str(l)] = v['dbC'+str(l)] = dbC\n",
        "  \n",
        "  return v,s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRsqK1dBrA0f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid(Z):\n",
        "  \n",
        "  A = 1/(1+np.exp(-Z))\n",
        "  return A"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Yo7WGB5rOOL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def relu(Z):\n",
        "  \n",
        "  A = np.maximum(Z,0.0001)\n",
        "  return A"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71T1zhkmrP-X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softmax(Z):\n",
        "  \n",
        "  exp = np.exp(Z)\n",
        "  expnorm = np.sum(exp,axis=0,keepdims=True)\n",
        "  A = exp/expnorm\n",
        "  assert(expnorm.shape == (1,exp.shape[1]))\n",
        "  \n",
        "  return A"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jx4rkJHLrSoR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Compute_Z(A,W,b):\n",
        "  \n",
        "  Z = np.dot(W,A) + b\n",
        "  return Z"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_29BnJ_rij4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Forward_Propagation_Helper(A,W,b,Y,B,activation):\n",
        "  \n",
        "  m_train = A.shape[1]\n",
        "  Z = Compute_Z(A,W,b)\n",
        "  mu = np.sum(Z,axis=1,keepdims=True)/m_train\n",
        "  Z_minus_mu = Z - mu\n",
        "  sigma = np.sqrt(np.sum(np.square(Z_minus_mu),axis=1,keepdims=True)/m_train)\n",
        "  ZNorm = Z_minus_mu/sigma\n",
        "  Zcap = Y*ZNorm + B\n",
        "  \n",
        "  if(activation == 'relu'):\n",
        "    A = relu(Zcap)\n",
        "  elif(activation == 'sigmoid'):\n",
        "    A = sigmoid(Zcap)\n",
        "  elif(activation == 'softmax'):\n",
        "    A = softmax(Zcap)\n",
        "   \n",
        "  return A,ZNorm,sigma"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBT3uWog2qR3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Zero_Pad(A,p):\n",
        "  \n",
        "  A_pad = np.pad(A,((0,0),(p[0],p[0]),(p[1],p[1]),(0,0)),'constant',constant_values=(0,0))\n",
        "  return A_pad"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCvKMQmw-AS4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Compute_Conv_Z(A_prev_slice,W,b):\n",
        "  \n",
        "  Z = np.sum(A_prev_slice*W) + b\n",
        "  return Z"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUsK88aP7z3T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Conv_Forward_Helper(A_prev,W,b,f,s,p,activation):\n",
        "  \n",
        "  (m,nH_prev,nW_prev,nC_prev) = A_prev.shape\n",
        "  nH = int((nH_prev+2*p[0]-f[0])/s[0])+1\n",
        "  nW = int((nW_prev+2*p[1]-f[1])/s[1])+1\n",
        "  nC = W.shape[0]\n",
        "  \n",
        "  #print(m,nH_prev,nW_prev,nC_prev)\n",
        "  Z = np.zeros((m,nH,nW,nC))\n",
        "  A_prev = Zero_Pad(A_prev,p)\n",
        "  \n",
        "  for i in range(m):\n",
        "    for h in range(nH):\n",
        "      for w in range(nW):\n",
        "        \n",
        "        vert_start = h*s[0]\n",
        "        vert_end = vert_start + f[0]\n",
        "        horiz_start = w*s[1]\n",
        "        horiz_end = horiz_start + f[1]\n",
        "        \n",
        "        A_prev_slice = A_prev[i,vert_start:vert_end,horiz_start:horiz_end,:]\n",
        "        \n",
        "        for c in range(nC):\n",
        "          Z[i,h,w,c] = Compute_Conv_Z(A_prev_slice,W[c,:,:,:],b[c,:,:,:])\n",
        "  \n",
        "  if(activation == 'relu'):\n",
        "    A = relu(Z)\n",
        "  else:\n",
        "    A = sigmoid(Z)\n",
        "    \n",
        "  return A"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZV1Az6XuC_bj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Pool_Forward_Helper(A_prev,f,s,pooltype):\n",
        "  \n",
        "  (m,nH_prev,nW_prev,nC_prev) = A_prev.shape\n",
        "  nH = int((nH_prev-f[0])/s[0])+1\n",
        "  nW = int((nW_prev-f[1])/s[1])+1\n",
        "  nC = nC_prev\n",
        "  \n",
        "  A = np.zeros((m,nH,nW,nC))\n",
        "  \n",
        "  for i in range(m):\n",
        "    for h in range(nH):\n",
        "      for w in range(nW):\n",
        "        for c in range(nC):\n",
        "          \n",
        "          vert_start = h*s[0]\n",
        "          vert_end = vert_start + f[0]\n",
        "          horiz_start = w*s[1]\n",
        "          horiz_end = horiz_start + f[1]\n",
        "          \n",
        "          A_prev_slice = A_prev[i,vert_start:vert_end,horiz_start:horiz_end,c]\n",
        "      \n",
        "          if(pooltype == 'max'):\n",
        "            A[i,w,h,c] = np.max(A_prev_slice)\n",
        "          else:\n",
        "            A[i,w,h,c] = np.average(A_prev_slice)\n",
        "  \n",
        "  return A"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgqbXvyarkz7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Forward_Propagation(train_set_X,parameters,keep_prob,layers_dims,filter_conv,filter_pool,stride_conv,stride_pool,padding):\n",
        "  \n",
        "  A = train_set_X\n",
        "  L = len(filter_conv)\n",
        "  activation = 'relu'\n",
        "  pooltype = 'max'\n",
        "  cache_C = {}\n",
        "  \n",
        "  for l in range(L):\n",
        "    cache_C['A'+str(l)] = A\n",
        "    A = Conv_Forward_Helper(A,parameters['WC'+str(l+1)],parameters['bC'+str(l+1)],filter_conv[l],stride_conv[l],padding[l],activation)\n",
        "    if(l == L-1):\n",
        "      cache_C['A'+str(L)] = A\n",
        "    if(l % 2 == 1):\n",
        "      cache_C['A_pool'+str(l)] = A\n",
        "      A = Pool_Forward_Helper(A,filter_pool[int(l/2)],stride_pool[int(l/2)],pooltype) \n",
        "      if(l == L-1):\n",
        "        cache_C['A_pool'+str(L)] = A\n",
        "  \n",
        "  L = len(layers_dims)-1\n",
        "  A = A.reshape((A.shape[0],A.shape[1]*A.shape[2]*A.shape[3])).T\n",
        "  cache_FC = {}\n",
        "  \n",
        "  for l in range(L):\n",
        "    t = l+1\n",
        "    cache_FC['A'+str(l)] = A\n",
        "    if(t < L):\n",
        "      activation = 'relu'\n",
        "    elif(parameters['WF'+str(L)].shape[0] > 1):\n",
        "      activation = 'softmax'\n",
        "    else :\n",
        "      activation = 'sigmoid'\n",
        "    A,ZNorm,sigma = Forward_Propagation_Helper(A,parameters['WF'+str(t)],parameters['bF'+str(t)],parameters['YF'+str(t)],parameters['BF'+str(t)],activation)\n",
        "    if (t != L):\n",
        "      P = np.random.randn(A.shape[0],A.shape[1])\n",
        "      P = 1-np.ceil(P-keep_prob[l])\n",
        "      A = A*P\n",
        "      A = A/keep_prob[l]\n",
        "      cache_FC['P'+str(t)] = P\n",
        "    cache_FC['ZNorm'+str(t)] = ZNorm\n",
        "    cache_FC['sigma'+str(t)] = sigma\n",
        "  cache_FC['A'+str(L)] = A\n",
        "  \n",
        "  return cache_C,cache_FC,A"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S87cvBvVrnsA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_cost_sigmoid(Y,A,parameters,regu,lambd,layers_dims):\n",
        "  \n",
        "  m_train = Y.shape[1]\n",
        "  cost = np.nansum(Y*np.log(A))+np.sum((1-Y)*np.log(1-A))\n",
        "  cost = -cost\n",
        "  \n",
        "  if(regu):\n",
        "    L = len(layers_dims)\n",
        "    sum = 0\n",
        "    for l in range(1,L):\n",
        "      WF = parameters['WF'+str(l)]\n",
        "      sum = sum + np.sum(np.square(WF))\n",
        "    sum = sum * (lambd*0.5/m_train)\n",
        "    cost = cost + sum\n",
        "  \n",
        "  return cost"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMF7-H25t0u9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_cost_softmax(Y,A,parameters,regu,lambd,layers_dims):\n",
        "  \n",
        "  m_train = Y.shape[1]\n",
        "  cost = np.sum(Y*np.log(A))\n",
        "  cost = -cost\n",
        "  \n",
        "  if(regu):\n",
        "    L = len(layers_dims)\n",
        "    sum = 0\n",
        "    for l in range(1,L):\n",
        "      WF = parameters['WF'+str(l)]\n",
        "      sum = sum + np.sum(np.square(WF))\n",
        "    sum = sum * (lambd*0.5/m_train)\n",
        "    cost = cost + sum\n",
        "   \n",
        "  return cost"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7xcq890uHTr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def relu_backward(dA,A):\n",
        "  \n",
        "  dZ = dA * np.int32(A>0)\n",
        "  return dZ"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJsTOHWxupM0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid_backward(dA,A):\n",
        "  \n",
        "  dZ = A*(1-A) * dA\n",
        "  return dZ"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49DeuVxkN0NB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Backward_Propagation_Helper(flag,dA,W,Y,A_prev,A_curr,ZNorm,sigma,activation,regu,lambd):\n",
        "  \n",
        "  if(flag):\n",
        "    dZcap = dA\n",
        "  else:\n",
        "    if(activation == 'relu'):\n",
        "      dZcap = relu_backward(dA,A_curr)\n",
        "    elif(activation == 'sigmoid'):\n",
        "      dZcap = sigmoid_backward(dA,A_curr)\n",
        "  \n",
        "  m_train = A_curr.shape[1]\n",
        "  dZNorm = Y * dZcap\n",
        "  dZ = dZNorm / sigma\n",
        "  dW = np.dot(dZ,A_prev.T)/m_train\n",
        "  if(regu):\n",
        "    dW = dW + W*lambd/m_train\n",
        "  db = np.sum(dZ,axis=1,keepdims=True)/m_train\n",
        "  dY = np.sum(dZcap * ZNorm,axis=1,keepdims=True)/m_train\n",
        "  dB = np.sum(dZcap,axis=1,keepdims=True)/m_train\n",
        "  dA = np.dot(W.T,dZ)\n",
        "  \n",
        "  assert(W.shape == dW.shape)\n",
        "  \n",
        "  return dA,dW,db,dY,dB"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oycHT9v0QVhm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Pool_Backward_Helper(dA,A_prev,f,s,pooltype):\n",
        "  \n",
        "  (m,nH_prev,nW_prev,nC_prev) = A_prev.shape\n",
        "  (m,nH,nW,nC) = dA.shape\n",
        " \n",
        "  #print(A_prev.shape)\n",
        "  dA_prev = np.zeros(A_prev.shape)\n",
        "  \n",
        "  for i in range(m):\n",
        "    for h in range(nH):\n",
        "      for w in range(nW):\n",
        "        for c in range(nC):\n",
        "          \n",
        "          vert_start = h*s[0]\n",
        "          vert_end = vert_start + f[0]\n",
        "          horiz_start = w*s[1]\n",
        "          horiz_end = horiz_start + f[1]\n",
        "          \n",
        "          A_prev_slice = A_prev[i,vert_start:vert_end,horiz_start:horiz_end,c]\n",
        "          \n",
        "          if(pooltype == 'max'):\n",
        "            mask = A_prev_slice == np.max(A_prev_slice)\n",
        "            dA_prev[i,vert_start:vert_end,horiz_start:horiz_end,c] += mask*dA[i,h,w,c]\n",
        "          elif(pooltype == 'average'):\n",
        "            mask = np.ones((f[0],f[1]))\n",
        "            dA_prev[i,vert_start:vert_end,horiz_start:horiz_end,c] += mask*dA[i,h,w,c]\n",
        "  \n",
        "  \n",
        "  return dA_prev"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCe8zhzjQYoe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Conv_Backward_Helper(dA,W,A_prev,A_curr,activation,f,s,p):\n",
        "  \n",
        "  if(activation == 'relu'):\n",
        "    dZ = relu_backward(dA,A_curr)\n",
        "  else:\n",
        "    dZ = sigmoid_backward(dA,A_curr)\n",
        "   \n",
        "  (m,nH_prev,nW_prev,nC_prev) = A_prev.shape\n",
        "  (m,nH,nW,nC) = dZ.shape\n",
        "  \n",
        "  dA = np.zeros(A_prev.shape)\n",
        "  dWC = np.zeros((nC,f[0],f[1],nC_prev))\n",
        "  dbC = np.zeros((nC,1,1,1))\n",
        "  \n",
        "  A_prev = Zero_Pad(A_prev,p)\n",
        "  dA = Zero_Pad(dA,p)\n",
        "  #print(dA.shape,A_prev.shape,p)\n",
        "  \n",
        "  for i in range(m):\n",
        "    for h in range(nH):\n",
        "      for w in range(nW):\n",
        "        \n",
        "        vert_start = h*s[0]\n",
        "        vert_end = vert_start + f[0]\n",
        "        horiz_start = w*s[1]\n",
        "        horiz_end = horiz_start + f[1]\n",
        "        \n",
        "        A_prev_slice = A_prev[i,vert_start:vert_end,horiz_start:horiz_end,:]\n",
        "        \n",
        "        for c in range(nC):\n",
        "          dA[i,vert_start:vert_end,horiz_start:horiz_end,:] += dZ[i,h,w,c]*W[c,:,:,:]\n",
        "          dWC[c,:,:,:] += A_prev_slice*dZ[i,h,w,c]\n",
        "          dbC[c,:,:,:] += dZ[i,h,w,c]\n",
        "  \n",
        "  if(p[0]!=0 and p[1]!=0):\n",
        "    dA = dA[:,p[0]:-p[0],p[1]:-p[1],:]\n",
        "  \n",
        "  assert(dA.shape == (m,nH_prev,nW_prev,nC_prev))\n",
        "  \n",
        "  return dA,dWC,dbC"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWLTPRrkOBo9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Backward_Propagation(train_set_Y,A,parameters,cache_FC,cache_C,keep_prob,regu,lambd,filter_conv,filter_pool,stride_conv,stride_pool,padding,layers_dims):\n",
        "  \n",
        "  grads = {}\n",
        "  L = len(layers_dims)-1\n",
        "  activation = 'relu'\n",
        "  pooltype = 'max'\n",
        "  \n",
        "  dA = A - train_set_Y\n",
        "  for l in reversed(range(L)):\n",
        "    if(l == L-1):\n",
        "      flag = True\n",
        "    else:\n",
        "      flag = False\n",
        "      dA = dA * cache_FC['P'+str(l+1)]\n",
        "      dA = dA/keep_prob[l]\n",
        "\n",
        "    dA,dWF,dbF,dYF,dBF = Backward_Propagation_Helper(flag,dA,parameters['WF'+str(l+1)],parameters['YF'+str(l+1)],cache_FC['A'+str(l)],cache_FC['A'+str(l+1)],cache_FC['ZNorm'+str(l+1)],cache_FC['sigma'+str(l+1)],activation,regu,lambd)\n",
        "    grads['dWF'+str(l+1)] = dWF\n",
        "    grads['dbF'+str(l+1)] = dbF\n",
        "    grads['dYF'+str(l+1)] = dYF\n",
        "    grads['dBF'+str(l+1)] = dBF\n",
        "  \n",
        "  L = len(filter_conv)\n",
        "  dA = dA.T\n",
        "  shape = cache_C['A_pool'+str(L)].shape\n",
        "  dA = dA.reshape(dA.shape[0],shape[3],shape[1],shape[2])\n",
        "  dA = np.moveaxis(dA,1,3)\n",
        "  \n",
        "  for l in reversed(range(L)):\n",
        "    if(l % 2 == 1):\n",
        "      dA = Pool_Backward_Helper(dA,cache_C['A_pool'+str(l)],filter_pool[int(l/2)],stride_pool[int(l/2)],pooltype)\n",
        "    dA,dWC,dbC = Conv_Backward_Helper(dA,parameters['WC'+str(l+1)],cache_C['A'+str(l)],cache_C['A'+str(l+1)],activation,filter_conv[l],stride_conv[l],padding[l])\n",
        "    grads['dWC'+str(l+1)] = dWC\n",
        "    grads['dbC'+str(l+1)] = dbC\n",
        "    \n",
        "  return grads"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7s5kgFzp-um",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Update_Parameters(grads,parameters,learning_rate,v,s,beta1,beta2,t,layers_dims,channels,epsilon=1e-8):\n",
        "\n",
        "  L = len(layers_dims)\n",
        "  \n",
        "  for l in range(1,L):\n",
        "    v['dWF'+str(l)] = beta1*v['dWF'+str(l)] + (1-beta1)*grads['dWF'+str(l)]\n",
        "    vdWF_corr = v['dWF'+str(l)]/(1-(beta1**t))\n",
        "    v['dbF'+str(l)] = beta1*v['dbF'+str(l)] + (1-beta1)*grads['dbF'+str(l)] \n",
        "    vdbF_corr = v['dbF'+str(l)]/(1-(beta1**t))\n",
        "    v['dYF'+str(l)] = beta1*v['dYF'+str(l)] + (1-beta1)*grads['dYF'+str(l)]\n",
        "    vdYF_corr = v['dYF'+str(l)]/(1-(beta1**t))\n",
        "    v['dBF'+str(l)] = beta1*v['dBF'+str(l)] + (1-beta1)*grads['dBF'+str(l)]\n",
        "    vdBF_corr = v['dBF'+str(l)]/(1-(beta1**t))\n",
        "\n",
        "    s['dWF'+str(l)] = beta2*s['dWF'+str(l)] + (1-beta2)*(grads['dWF'+str(l)]**2)\n",
        "    sdWF_corr = s['dWF'+str(l)]/(1-(beta2**t))\n",
        "    s['dbF'+str(l)] = beta2*s['dbF'+str(l)] + (1-beta2)*(grads['dbF'+str(l)]**2)\n",
        "    sdbF_corr = s['dbF'+str(l)]/(1-(beta2**t))\n",
        "    s['dYF'+str(l)] = beta2*s['dYF'+str(l)] + (1-beta2)*(grads['dYF'+str(l)]**2)\n",
        "    sdYF_corr = s['dYF'+str(l)]/(1-(beta2**t))\n",
        "    s['dBF'+str(l)] = beta2*s['dBF'+str(l)] + (1-beta2)*(grads['dBF'+str(l)]**2)\n",
        "    sdBF_corr = s['dBF'+str(l)]/(1-(beta2**t))\n",
        "    \n",
        "    parameters['WF'+str(l)] = parameters['WF'+str(l)] - learning_rate*vdWF_corr/(np.sqrt(sdWF_corr)+epsilon)\n",
        "    parameters['bF'+str(l)] = parameters['bF'+str(l)] - learning_rate*vdbF_corr/(np.sqrt(sdbF_corr)+epsilon)\n",
        "    parameters['YF'+str(l)] = parameters['YF'+str(l)] - learning_rate*vdYF_corr/(np.sqrt(sdYF_corr)+epsilon)\n",
        "    parameters['BF'+str(l)] = parameters['BF'+str(l)] - learning_rate*vdBF_corr/(np.sqrt(sdBF_corr)+epsilon)\n",
        "\n",
        "  L = len(channels)\n",
        "  \n",
        "  for l in range(1,L):\n",
        "    v['dWC'+str(l)] = beta1*v['dWC'+str(l)] + (1-beta1)*grads['dWC'+str(l)]\n",
        "    vdWC_corr = v['dWC'+str(l)]/(1-(beta1**t))\n",
        "    v['dbC'+str(l)] = beta1*v['dbC'+str(l)] + (1-beta1)*grads['dbC'+str(l)]\n",
        "    vdbC_corr = v['dbC'+str(l)]/(1-(beta1)**t)\n",
        "    \n",
        "    s['dWC'+str(l)] = beta2*s['dWC'+str(l)] + (1-beta2)*(grads['dWC'+str(l)]**2)\n",
        "    sdWC_corr = s['dWC'+str(l)]/(1-(beta2**t))\n",
        "    s['dbC'+str(l)] = beta2*s['dbC'+str(l)] + (1-beta2)*(grads['dbC'+str(l)]**2)\n",
        "    sdbC_corr = s['dbC'+str(l)]/(1-(beta2**t))\n",
        "    \n",
        "    parameters['WC'+str(l)] -= learning_rate*vdWC_corr/(np.sqrt(sdWC_corr)+epsilon)\n",
        "    parameters['bC'+str(l)] -= learning_rate*vdbC_corr/(np.sqrt(sdbC_corr)+epsilon)\n",
        "  \n",
        "  return parameters,v,s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdOQod01vRuW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Shuffle_And_Split(train_set_X,train_set_Y,mini_batch_size):\n",
        "  \n",
        "  m_train = train_set_X.shape[0]\n",
        "  perm = list(np.random.permutation(m_train))\n",
        "  train_set_X = train_set_X[perm,:,:,:]\n",
        "  train_set_Y = train_set_Y[:,perm]\n",
        "  \n",
        "  n = int(m_train/mini_batch_size)\n",
        "  minibatches = []\n",
        "  \n",
        "  for i in range(n):\n",
        "    X = train_set_X[i*mini_batch_size:(i+1)*mini_batch_size,:,:,:]\n",
        "    Y = train_set_Y[:,i*mini_batch_size:(i+1)*mini_batch_size]\n",
        "    minibatch = (X,Y)\n",
        "    minibatches.append(minibatch)\n",
        "  \n",
        "  if(m_train % mini_batch_size != 0):\n",
        "    X = train_set_X[n*mini_batch_size:m_train,:,:,:]\n",
        "    Y = train_set_Y[:,n*mini_batch_size:m_train]\n",
        "    minibatch = (X,Y)\n",
        "    minibatches.append(minibatch)\n",
        "  \n",
        "  return minibatches"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vySjoNTQvXnz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Predict(X,parameters,keep_prob,layers_dims,filter_conv,filter_pool,stride_conv,stride_pool,padding):\n",
        "  \n",
        "  cache_C,cache_FC,A = Forward_Propagation(X,parameters,keep_prob,layers_dims,filter_conv,filter_pool,stride_conv,stride_pool,padding)\n",
        "  L = len(layers_dims)-1\n",
        "  \n",
        "  if(parameters['WF'+str(L)].shape[0] == 1):\n",
        "    A = np.abs(np.ceil(A-0.5))\n",
        "    A = A.astype('int32')\n",
        "  else:\n",
        "    A = np.argmax(A,axis=0)\n",
        "    A = np.eye(10)[A][0].T\n",
        "   \n",
        "  return A"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMYliRz8vbJX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Model(train_set_X,train_set_Y,test_set_X,test_set_Y,learning_rate,epochs,layers_dims,keep_prob,channels,filter_conv,filter_pool,stride_conv,stride_pool,padding):\n",
        "  \n",
        "  beta1 = 0.8\n",
        "  beta2 = 0.9\n",
        "  beta3 = 0.99\n",
        "  lambd = 0.7\n",
        "  regu = False\n",
        "  mini_batch_size = 25\n",
        "  \n",
        "  L = len(layers_dims)\n",
        "  if(layers_dims[L-1] > 1):\n",
        "    train_set_Y = np.eye(10)[train_set_Y][0].T\n",
        "    test_set_Y = np.eye(10)[test_set_Y][0].T\n",
        "    \n",
        "  total_cost = []\n",
        "  parameters = Initialize_Parameters(layers_dims,filter_conv,channels)\n",
        "  v,s = Initialize_Optimizer(layers_dims,parameters,filter_conv,channels)\n",
        "  \n",
        "  for i in range(epochs):\n",
        "    minibatches = Shuffle_And_Split(train_set_X,train_set_Y,mini_batch_size)\n",
        "    for minibatch in minibatches:\n",
        "      (X,Y) = minibatch\n",
        "      cache_C,cache_FC,A = Forward_Propagation(X,parameters,keep_prob,layers_dims,filter_conv,filter_pool,stride_conv,stride_pool,padding)\n",
        "      \n",
        "      if(parameters['WF'+str(L-1)].shape[0] > 1):\n",
        "        cost = compute_cost_softmax(Y,A,parameters,regu,lambd,layers_dims)\n",
        "      else :\n",
        "        cost = compute_cost_sigmoid(Y,A,parameters,regu,lambd,layers_dims)\n",
        "      \n",
        "      grads = Backward_Propagation(Y,A,parameters,cache_FC,cache_C,keep_prob,regu,lambd,filter_conv,filter_pool,stride_conv,stride_pool,padding,layers_dims)\n",
        "      \n",
        "      #if (i % 10000 == 0):\n",
        "        #flag=Gradient_Checking(grads,parameters,X,Y,keep_prob=[1,1,1],regu=regu,lambd=lambd)\n",
        "        #print(\"Result of Gradient Checking : \"+str(flag))\n",
        "        \n",
        "      parameters,v,s = Update_Parameters(grads,parameters,learning_rate,v,s,beta2,beta3,i+1,layers_dims,channels)\n",
        "      \n",
        "      if(True):\n",
        "        print('Cost after '+str(i)+'th Iteration : '+str(cost))\n",
        "      \n",
        "    if(i % 500 == 0):\n",
        "        total_cost.append(cost)\n",
        "     \n",
        "  A_train = Predict(train_set_X,parameters,keep_prob,layers_dims,filter_conv,filter_pool,stride_conv,stride_pool,padding)\n",
        "  A_test = Predict(test_set_X,parameters,keep_prob,layers_dims,filter_conv,filter_pool,stride_conv,stride_pool,padding)\n",
        "  \n",
        "  if(parameters['WF'+str(L-1)].shape[0] == 1):\n",
        "    train_acc = 100-np.mean(np.abs(train_set_Y-A_train))*100\n",
        "    test_acc = 100-np.mean(np.abs(test_set_Y-A_test))*100\n",
        "    \n",
        "  elif(parameters['WF'+str(L-1)].shape[0] > 1):\n",
        "    train_temp = np.equal(np.argmax(A_train,axis=0),np.argmax(train_set_Y,axis=0))\n",
        "    train_acc = np.mean(train_temp.astype('int32'))*100\n",
        "    test_temp = np.equal(np.argmax(A_test,axis=0),np.argmax(test_set_Y,axis=0))\n",
        "    test_acc = np.mean(test_temp.astype('int32'))*100\n",
        "  \n",
        "  print(\"Training Set Accuracy : \"+str(train_acc))\n",
        "  print(\"Test Set Accuracy : \"+str(test_acc))\n",
        "  \n",
        "  plt.plot(total_cost)\n",
        "  plt.xlabel('Iterations')\n",
        "  plt.ylabel('Cost')\n",
        "  plt.title(\"Learning_Rate : \"+str(learning_rate))\n",
        "  plt.show()  \n",
        "  \n",
        "  return parameters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XyRP2ihB1cph",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Calculate_HW(train_set_X,filter_conv,filter_pool,stride_conv,stride_pool,padding,channels):\n",
        "  \n",
        "  shape = train_set_X.shape\n",
        "  h = shape[1]\n",
        "  w = shape[2]\n",
        "  L = len(filter_conv)\n",
        "  \n",
        "  for l in range(L):\n",
        "    h = int((h+2*padding[l][0]-filter_conv[l][0])/stride_conv[l][0]) + 1\n",
        "    w = int((w+2*padding[l][1]-filter_conv[l][0])/stride_conv[l][1]) + 1\n",
        "    if(l%2 == 1):\n",
        "      h = int((h-filter_pool[int(l/2)][0])/stride_pool[int(l/2)][0]) + 1\n",
        "      w = int((w-filter_pool[int(l/2)][1])/stride_pool[int(l/2)][1]) + 1\n",
        "  \n",
        "  c = channels[L]\n",
        "  \n",
        "  return h*w*c"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AEX5B5Z0r6p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Caller_Function():\n",
        "  \n",
        "  train_set_X,train_set_Y,test_set_X,test_set_Y = Data_Preprocessing()\n",
        "  learning_rate = 0.3\n",
        "  epochs = 5\n",
        "  keep_prob = [1,1]\n",
        "  channels = [train_set_X.shape[3]]\n",
        "  channels.extend([32,64])\n",
        "  filter_conv = [(3,3),(3,3)]\n",
        "  filter_pool = [(2,2)]\n",
        "  stride_conv = [(1,1),(1,1)]\n",
        "  stride_pool = [(2,2)]\n",
        "  padding = [(0,0),(0,0)]\n",
        "  layers_dims = [Calculate_HW(train_set_X,filter_conv,filter_pool,stride_conv,stride_pool,padding,channels)]\n",
        "  layers_dims.extend([1024,10])\n",
        "  parameters = Model(train_set_X,train_set_Y,test_set_X,test_set_Y,learning_rate,epochs,layers_dims,keep_prob,channels,filter_conv,filter_pool,stride_conv,stride_pool,padding)\n",
        "  \n",
        "  for key,value in parameters.items():\n",
        "    print(key,value)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDkXe8Pi5VAs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "27aff615-b786-4dec-c889-33115cbafcf4"
      },
      "source": [
        "Caller_Function()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cost after 0th Iteration : 57.56111099455515\n",
            "Cost after 0th Iteration : 61.56348552197319\n",
            "Cost after 1th Iteration : 54.5281873320763\n",
            "Cost after 1th Iteration : 55.536910240382205\n",
            "Cost after 2th Iteration : 44.87751321845079\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7s3SRtFYfJBf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}